{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Princewzz/Machine-learning-cousework/blob/main/coursework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VLynVzhLqp2"
      },
      "source": [
        "# Coursework 1 - Mathematics for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhT5XP_6Lqp6"
      },
      "source": [
        "## CID: 02016491\n",
        "\n",
        "**Colab link:** insert colab link here\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KwaXeFFLqp7"
      },
      "source": [
        "## Exercise 1: Quick questions [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM7VjcflLqp7"
      },
      "source": [
        "#### Question 1:\n",
        "\n",
        "The risk, or true risk, measures the expected loss of a chosen loss function over the data distribution.\n",
        "$$R(f) := \\mathbb{E}_{D} [L(f(x), y)]$$\n",
        "The empirical risk is the estimate of the true risk based on the observed data.\n",
        "$$\\hat{R}(f) := \\frac{1}{N} \\sum_{i=1}^{N} L(f(x_i), y_i)$$\n",
        " Since the data distribution is unknown, true risk is also unknow, while empirical risk can be calculated after observing data samples from the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6QMtYyzLqp7"
      },
      "source": [
        "#### Question 2:\n",
        "Trainging error is the empirical risk computed on the training datasrt. Generalization error is the difference between true risk and empirical risk on new, unseen data, normally the validation set. Since true risk is unknown, it is approximated by the dfference between training empirical risk and test empirical risk. S small generalization error indicates that the model is performing well, however a small training error does not necessarilly suggest a well fit, since a small training error with a large generalization error suggests overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SurhvQoiLqp8"
      },
      "source": [
        "#### Question 3:\n",
        "We need independence for the inequality to hold in the following step of the proof\n",
        "$$\n",
        "\\begin{align*}\n",
        "e^{-te} \\prod_{i=1}^m \\mathbb{E}\\left[e^{t(X_i - \\mathbb{E}[X_i])}\\right] &\\leq e^{-te} \\prod_{i=1}^m e^{\\frac{t^2(b_i - a_i)^2}{8}}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3PQ0SX8Lqp8"
      },
      "source": [
        "#### Question 4:\n",
        "If the dataset is relatively small, for instance 100 data, then we could split the dataset into:\n",
        "- **Training set (70%)**: Used to train both hypothesis classes \\( H1 \\) and \\( H2 \\).\n",
        "- **Validation set (15%)**: Used for hyperparameter tuning.\n",
        "- **Test set (15%)**: Used to evaluate the final performance.\n",
        "\n",
        "As the data size is increases, we can relatively reduce the percentage used in calidation and test. For instance if we had a million data points, then we could do **Training set (90%)**, **Validation set (5%)** and **Test set (5%)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbNgvh81Lqp8"
      },
      "source": [
        "#### Question 5:\n",
        "\n",
        "The Rademacher complexity of a function class \\( G \\) is defined as:\n",
        "$$\n",
        "\\mathcal{R}_m(G) = \\mathbb{E}_{S \\sim D^m} \\left[ \\hat{\\mathcal{R}}_S(G) \\right]\n",
        "$$\n",
        "\n",
        "where $$ \\hat{\\mathcal{R}}_S(G) = \\mathbb{E}_{\\sigma} \\left[ \\sup_{g \\in G} \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_i g(z_i) \\right] $$ is the empirical Rademacher complexity on a sample \\( S \\).\n",
        "generalization bounds in 1.4 includes Hoeffding's and McDiarmid's.\n",
        "Rademacher complexity is data-dependent, while the generalization bounds are worst-case and distribution-independent.\n",
        "This gives Rademacher complexity some key advantages such as tighter bounds, better for complex models and more adaptive to data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jK69iEyLqp8"
      },
      "source": [
        "#### Question 6:\n",
        "On top of basic gradient descent, gradient descent with momentum introduces a momentum term, which helps smooth updates and accelerates convergence. The update rules are:\n",
        "$$x_{k+1} = x_k - \\alpha z_{k+1}$$\n",
        "$$z_{k+1} = \\beta z_k + \\nabla f(x_k)$$\n",
        "where $\\beta$ is a hyper parameter. The intuition is similar to a snowball rolling downhill—it gains momentum on steep slopes, making it move faster, while in oscillatory regions, it smooths out updates and prevents excessive zig-zagging. This helps speed up convergence and improves stability compared to standard gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u2Mn9o0Lqp9"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Exercise 2: Statistical learning theory [7 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4vZVxiULqp9"
      },
      "source": [
        "\n",
        "### Question 1 [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVmqmAvoLqp9"
      },
      "source": [
        "### Risk Minimizer and Empirical Risk  \n",
        "\n",
        "The risk minimizer $ f_{S_1, k} $ under the hypothesis class $ F_k $ is defined as:\n",
        "\n",
        "$$\n",
        "f_{S_1, k} = \\arg\\min_{f \\in \\mathcal{F}_k} \\hat{R}_{S_1}(f)\n",
        "$$\n",
        "\n",
        "where the **empirical risk** $\\hat{R}_{S_1}(f) $ is given by:\n",
        "\n",
        "$$\n",
        "\\hat{R}_{S_1}(f) = \\frac{1}{(1 - \\alpha)L} \\sum_{(x_i, y_i) \\in S_1} L(f(x^i), y^i)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ |S_1| = (1 - \\alpha)N $ is the size of the training dataset.\n",
        "- $ L(f(x_i), y_i) $ is the loss function measuring the error between prediction and ground truth.\n",
        "- $ \\mathcal{F}_k $ is a hypothesis class containing functions $ f: X \\to \\{0,1\\} $.\n",
        "\n",
        "The function $\\mathcal{F}_{S_1, k} $ is the best hypothesis in $ \\mathcal{F}_k $ that **minimizes empirical risk** on the training dataset $ S_1 $.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUTNIxVGLqp9"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2 [1 point]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LersF1T2btQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP432HR6Lqp9"
      },
      "source": [
        "$$\n",
        "\\sup_{k \\geq 1} \\left| R(f_{S_1, k}) - \\hat{R}_{S_2}(f_{S_1, k}) \\right| > \\epsilon$$\n",
        "By the definition of supremum, this means that there exist at least one k such that $R(f_{S_1, k}) - \\hat{R}_{S_2}(f_{S_1, k})$ is greater than $\\epsilon$\n",
        "Hence the probability that the supremum exceeds\n",
        "$\\epsilon$ is at most the sum of probabilities that each individual term exceeds $\\epsilon$:\n",
        "$$\n",
        "P \\left( \\sup_{k \\geq 1} \\left| R(f_{S_1, k}) - \\hat{R}_{S_2}(f_{S_1, k}) \\right| > \\epsilon \\right)\n",
        "\\leq \\sum_{k=1}^{\\infty} P \\left( \\left| R(f_{S_1, k}) - \\hat{R}_{S_2}(f_{S_1, k}) \\right| \\geq \\epsilon \\right).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5lWq_wULqp9"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 3 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfV3Iw35Lqp-"
      },
      "source": [
        "since $P(A) = E(1_A)$\n",
        "$$\n",
        "P \\left( R(f_{S_1,k}) - \\hat{R}_{S_2}(f_{S_1,k}) \\geq \\epsilon \\right) = \\mathbb{E} \\left[ \\mathbf{1} \\left( R(f_{S_1,k}) - \\hat{R}_{S_2}(f_{S_1,k}) \\geq \\epsilon \\right) \\right].\n",
        "$$\n",
        "By the law of total expectation $E[X]=E[E[X∣Y]]$\n",
        "$$\n",
        "P \\left( R(f_{S_1,k}) - \\hat{R}_{S_2}(f_{S_1,k}) \\geq \\epsilon \\right)\n",
        "= \\mathbb{E} \\left[ \\mathbb{E} \\left[ \\mathbf{1} \\left( R(f_{S_1,k}) - \\hat{R}_{S_2}(f_{S_1,k}) \\geq \\epsilon \\right) \\mid S_1 \\right] \\right]\n",
        "= \\mathbb{E} \\left[ P \\left( R(f_{S_1,k}) - \\hat{R}_{S_2}(f_{S_1,k}) \\geq \\epsilon \\mid S_1 \\right) \\right]\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kh7izoELqp-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 4 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2WS9I0yLqp-"
      },
      "source": [
        "By Hoeffding's Inequality:\n",
        "$$\n",
        "P \\left[ R(f_{S_1,k}) - \\hat{R}_{S_2}(f_{S_1,k}) \\geq \\epsilon | S_1\\right] \\leq e^{-2\\epsilon^2  \\alpha N}\n",
        "$$\n",
        "since\n",
        "$$\n",
        "\\hat{R}_{S_2}(f_{S_1,k}) = \\frac{1}{\\alpha N} \\sum_{(x_i, y_i) \\in S_2} L(f(x^i), y^i)\n",
        "$$\n",
        " $$L \\in \\{0, 1\\}$$  $$ |S_2| = \\alpha N $$\n",
        "$$\n",
        "R(f_{S_1,k}) = \\mathbb{E} \\left[ \\hat{R}_{S_2}(f_{S_1,k})\\right]\n",
        "$$\n",
        "or directly by Corollary 1.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2iPz5TjLqp-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 5 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeZPJw3ZLqp-"
      },
      "source": [
        "From question 4 we have that\n",
        "$$\n",
        "P \\left( R(f_{S_1,k}) - \\hat{R}_{S_2}(f_{S_1,k}) \\geq \\epsilon + \\sqrt{\\frac{\\log k}{\\alpha N}} \\mid S_1 \\right)  \n",
        "\\leq \\exp \\left[ -2\\alpha N \\left(\\epsilon + \\sqrt{\\frac{\\log k}{\\alpha N}} \\right)^2 \\right]  \n",
        "$$  \n",
        "\n",
        "$$\n",
        "=  \\exp \\left[ -2\\alpha N \\left(\\epsilon^2 + 2\\epsilon \\sqrt{\\frac{\\log k}{\\alpha N}} + \\frac{\\log k}{\\alpha N} \\right) \\right]  \n",
        "$$  \n",
        "\n",
        "$$\n",
        "\\leq \\exp \\left[ -2\\alpha N \\left(\\epsilon^2 + \\frac{\\log k}{\\alpha N} \\right) \\right]  \n",
        "$$  \n",
        "\n",
        "$$\n",
        "= \\frac{1}{k^2} \\exp(-2\\alpha N \\epsilon^2)  \n",
        "$$\n",
        "$$\n",
        "\\leq \\frac{2}{k^2} \\exp(-2\\alpha N \\epsilon^2)  \n",
        "$$\n",
        "since we have that\n",
        "$$\n",
        " \\alpha N \\epsilon \\sqrt{\\frac{\\log k}{\\alpha N}}  \\geq 0\n",
        "$$\n",
        "since all terms are non-negative\n",
        "$$\n",
        "\\exp \\left( -4\\alpha N \\epsilon \\sqrt{\\frac{\\log k}{\\alpha N}} \\right) \\leq 1\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPVX-co3Lqp-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 6 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shSc5i72Lqp-"
      },
      "source": [
        "Substituting the equation deduced in question 5 into the equation deduced in question 2 and sbstituting $ϵ$ to $\\epsilon + \\sqrt{\\frac{\\log k}{\\alpha N}}$, we have that\n",
        "$$\n",
        "\\mathbb{P}\\left[\\sup_{k\\geq 1}\\left\\{|R(f_{S_1,k})-\\hat{R}_{S_2}(f_{S_1,k})|-\\sqrt{\\frac{\\log k}{\\alpha N}}\\right\\}>\\epsilon\\right]\\leq \\frac{2}{k^2}e^{-2\\alpha N\\epsilon^2} \\leq 4e^{-2\\alpha N\\epsilon^2}\n",
        "$$\n",
        "since $k \\geq 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNEdDG6RLqp-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 7 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0FaJ7VLqp-"
      },
      "source": [
        "By definition of $\\tilde{f}_{S_2}$ is the best performing $f_{S_1,k}$ heBY chosing $k = \\tilde{k}$, ie, the index of the best-performing model on $S_2$,  we have following from the last quyestion that\n",
        "$$\n",
        "R(\\tilde{f}_{S_2}) \\leq \\hat{R}_{S_2}(\\tilde{f}_{S_2}) + \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}} + \\epsilon\n",
        "$$\n",
        "with probability at least $1-4e^{-2\\alpha N\\epsilon^2}$\n",
        "To ensure that this holds with probability at least probability $1 - \\delta $, we set\n",
        "$$\n",
        "4e^{-2\\alpha N\\epsilon^2} = \\delta\n",
        "$$\n",
        "Hence we have that\n",
        "$$\n",
        "ϵ = \\sqrt{\\frac{\\log (4/\\delta )}{2 \\alpha N}}\n",
        "$$\n",
        "substituting in we have\n",
        "$$\n",
        "R(\\tilde{f}_{S_2}) \\leq \\hat{R}_{S_2}(\\tilde{f}_{S_2}) + \\sqrt{\\frac{\\log \\tilde{k}}{\\alpha N}} + \\sqrt{\\frac{\\log (4/\\delta )}{2 \\alpha N}}\n",
        "$$\n",
        "as desired"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuJhdqclLqp-"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Exercise 3: Optimization questions [4.5 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51UQAGf2Lqp-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 1 [0.5 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Fl4wyHLqp_"
      },
      "source": [
        "Define\n",
        "$$\n",
        "f(x) = \\| x \\|\n",
        "$$\n",
        "to prove that $f(x)$ is convex, for $λ ∈ [0, 1]$\n",
        "$$\n",
        "f(\\lambda x + (1 - \\lambda)y)\n",
        "= \\| \\lambda x + (1 - \\lambda)y \\|\n",
        "$$by triangle inequality$$\n",
        "\\leq \\| \\lambda x \\| + \\|  (1 - \\lambda)y \\|\n",
        "$$since $λ$ in non-negative$$\n",
        "= \\lambda \\| x \\| + (1 - \\lambda) \\| y \\|\n",
        "$$$$\n",
        "= \\lambda f(x) + (1 - \\lambda) f(y)\n",
        "$$\n",
        "hence by definition of convexity $f(x)$ is convex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJsJNE9HLqp_"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2 [0.5 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKMaMQFQLqp_"
      },
      "source": [
        "Given that $f(x)$ is convex, for $λ ∈ [0, 1]$\n",
        "$$\n",
        "f(\\lambda x + (1 - \\lambda)y) \\leq \\lambda f(x) + (1 - \\lambda) f(y)\n",
        "$$\n",
        "$$\n",
        "g(\\lambda x + (1 - \\lambda)y)\n",
        "$$$$\n",
        "=f(A(\\lambda x + (1 - \\lambda)y) + b)\n",
        "$$$$\n",
        "=f[(\\lambda (Ax + b) + (1 - \\lambda)(Ay + b)]\n",
        "$$$$\n",
        "\\leq \\lambda f(Ax + b) + (1 - \\lambda) f(Ay + b)\n",
        "$$$$\n",
        "= \\lambda g(x) + (1 - \\lambda) g(y)\n",
        "$$\n",
        "Thus, $g(x)$ is convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0tR6BnALqp_"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 3 [0.5 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc3tBlIcLqp_"
      },
      "source": [
        "Given that $f_1(x)$ and $f_2(x)$ are convex, for $\\lambda \\in [0, 1]$, we have\n",
        "$$\n",
        "f_1(\\lambda x + (1 - \\lambda)y) \\leq \\lambda f_1(x) + (1 - \\lambda) f_1(y)\n",
        "$$\n",
        "$$\n",
        "f_2(\\lambda x + (1 - \\lambda)y) \\leq \\lambda f_2(x) + (1 - \\lambda) f_2(y)\n",
        "$$\n",
        "Now, consider the function $h(x) = f_1(x) + f_2(x)$:\n",
        "$$\n",
        "h(\\lambda x + (1 - \\lambda)y)\n",
        "$$$$\n",
        "= f_1(\\lambda x + (1 - \\lambda)y) + f_2(\\lambda x + (1 - \\lambda)y)\n",
        "$$$$\n",
        "\\leq \\lambda f_1(x) + (1 - \\lambda) f_1(y) + \\lambda f_2(x) + (1 - \\lambda) f_2(y)\n",
        "$$$$\n",
        "= \\lambda (f_1(x) + f_2(x)) + (1 - \\lambda)(f_1(y) + f_2(y))\n",
        "$$$$\n",
        "= \\lambda h(x) + (1 - \\lambda) h(y)\n",
        "$$\n",
        "Thus, $h(x)$ is convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3Fo5A8qLqp_"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 4 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE0G5jsQLqp_"
      },
      "source": [
        "\n",
        "Consider the function $\\phi: \\mathbb{R} \\to \\mathbb{R}$ given by:  \n",
        "$$\n",
        "\\phi(z) = \\ln(1 + \\exp(-z)).\n",
        "$$  \n",
        "To show that $f(x)$ is convex, it is sufficient to show that $\\phi(z)$ is convex and that its argument $z = y_i \\langle x, a_i \\rangle$ is affine in $x$.\n",
        "\n",
        "We compute the first derivative of $\\phi(z)$:  \n",
        "$$\n",
        "\\phi'(z) = \\frac{-\\exp(-z)}{1 + \\exp(-z)} = -\\frac{1}{1 + \\exp(z)}.\n",
        "$$  \n",
        "Taking the second derivative:  \n",
        "$$\n",
        "\\phi''(z) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2} = \\frac{1}{(1 + \\exp(z))(1 + \\exp(-z))}.\n",
        "$$  \n",
        "Since $\\phi''(z) \\geq 0$ for all $z \\in \\mathbb{R}$, the function $\\phi(z)$ is convex.\n",
        "\n",
        "Since $\\phi(z)$ is convex and $z = y_i \\langle x, a_i \\rangle$ is an affine function of $x$, it follows that $\\phi(y_i \\langle x, a_i \\rangle)$ is convex in $x$. Followed by the last question where we proved the sum of convex functions is also convex hence we can conclude that $f(x)$ is convex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7_Avb9KLqp_"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 5 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq7Zw1_RLqp_"
      },
      "source": [
        "The gradient of $ f(x) $ is given by  \n",
        "$$ \\nabla f(x) = A^\\top (Ax - b). $$\n",
        "The Hessian of $ f(x) $ is  \n",
        "$$ \\nabla^2 f(x) = A^\\top A. $$  \n",
        "Since $ A $ has full column rank, the matrix $ A^\\top A $ is positive definite.\n",
        "$\\forall x, u \\in \\mathbb{R}^d $\n",
        "$$\n",
        "u^T \\nabla^2 f(x) u = u^T A^T A u\n",
        "\\geq λ_{min} (A^T A) \\|u\\|^2\n",
        "$$\n",
        "by Rayleigh's inequality\n",
        "\n",
        "Hence by Proposition 2.13. we have that $f(x)$ is $λ_{min} (A^T A)$-strong convex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SxFGyozc1zOV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ngLxsi40KPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0J6I2WbLqp_"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 6 [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnnilc-CLqqA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7kwfVDQLqqA"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Exercise 2: Gradient descent and linear regression [5.5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtHT5GvCLqqA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "%matplotlib inline\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "\n",
        "X = torch.tensor(diabetes.data)\n",
        "Y = torch.tensor(diabetes.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCTa3-GPLqqJ"
      },
      "source": [
        "***\n",
        "### Question 1 [0.5 point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_lX5SAsLqqJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiYWBpqDLqqK"
      },
      "source": [
        "***\n",
        "### Question 2 [0.5 point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-tSuY0GLqqK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2il23rNxLqqK"
      },
      "source": [
        "***\n",
        "### Question 3 [0.5 point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW6SX9P8LqqK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUrvF8qvLqqK"
      },
      "source": [
        "***\n",
        "### Question 4 [1 point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92xhraztLqqK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wyYuQFPLqqL"
      },
      "source": [
        "***\n",
        "### Question 5 [0.5 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43z7LSKsLqqL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfABm_WfLqqL"
      },
      "source": [
        "***\n",
        "### Question 6 [0.5 point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYpMBasNLqqL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMvyi65YLqqL"
      },
      "source": [
        "***\n",
        "### Question 7 [1 point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLo-rcb3LqqM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk4xyEuyLqqM"
      },
      "source": [
        "***\n",
        "### Question 8 [1 point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd8ga5nOLqqM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}